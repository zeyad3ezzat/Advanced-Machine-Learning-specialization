{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of practice_reinforce_pytorch.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce_pytorch.ipynb","timestamp":1600855624675}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m31gsGH_QPnk","colab_type":"text"},"source":["# REINFORCE in PyTorch\n","\n","Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"-4b3q76QQPnl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600927547900,"user_tz":-120,"elapsed":907,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"outputId":"9ef95f3c-44bb-4264-d348-255f3406116c"},"source":["import sys, os\n","if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","    !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Starting virtual X frame buffer: Xvfb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3A5aJJk2QPnr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927547901,"user_tz":-120,"elapsed":887,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MbRnAMyuQPnu","colab_type":"text"},"source":["A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."]},{"cell_type":"code","metadata":{"id":"MHnKPYDVQPnv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"ok","timestamp":1600927548452,"user_tz":-120,"elapsed":1419,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"outputId":"02c316a8-3acb-45b3-b4cf-908c2729d8ed"},"source":["env = gym.make(\"CartPole-v0\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f01fff96908>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATVklEQVR4nO3df6zd9X3f8ecL2xhaGMZwY1zb1LS4ZLRqDLklREETJUoKaBmJlEawjaAIyV1EpESKlkEnrYk0JKqkYYvWsbqCQhYGIT8hiC4lBKlDUyAmMcT8Ck7iDLs2NgQD5oeNzXt/3K/JAftyz/3F8eee50M6ut/v+/v5nvP+KIdXvv7c77knVYUkqR2HDboBSdLkGNyS1BiDW5IaY3BLUmMMbklqjMEtSY2ZteBOcm6Sx5JsTHL5bL2OJA2bzMZ93EnmAT8F3gdsBn4IXFRVD8/4i0nSkJmtK+4zgI1V9fOq2gPcDFwwS68lSUNl/iw97zLgiZ79zcC7xht8/PHH18qVK2epFUlqz6ZNm3jqqadysGOzFdwTSrIGWANw4oknsm7dukG1IkmHnNHR0XGPzdZSyRZgRc/+8q72mqpaW1WjVTU6MjIyS21I0twzW8H9Q2BVkpOSHA5cCNw2S68lSUNlVpZKqmpvkk8A3wXmAddV1UOz8VqSNGxmbY27qu4A7pit55ekYeUnJyWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNWZaX12WZBPwPLAP2FtVo0kWA18FVgKbgI9U1TPTa1OStN9MXHH/cVWtrqrRbv9y4K6qWgXc1e1LkmbIbCyVXADc0G3fAHxwFl5DkobWdIO7gH9Icn+SNV1tSVVt7ba3AUum+RqSpB7TWuMGzqqqLUneBtyZ5NHeg1VVSepgJ3ZBvwbgxBNPnGYbkjQ8pnXFXVVbup/bgW8BZwBPJlkK0P3cPs65a6tqtKpGR0ZGptOGJA2VKQd3kt9McvT+beD9wAbgNuCSbtglwK3TbVKS9GvTWSpZAnwryf7n+V9V9b+T/BC4JcmlwC+Bj0y/TUnSflMO7qr6OfCOg9SfBt47naYkSePzk5OS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYyYM7iTXJdmeZENPbXGSO5M83v08tqsnyZeSbEzyYJLTZ7N5SRpG/VxxXw+c+4ba5cBdVbUKuKvbBzgPWNU91gDXzEybkqT9JgzuqvpH4FdvKF8A3NBt3wB8sKf+5RrzA2BRkqUz1awkaepr3Euqamu3vQ1Y0m0vA57oGbe5qx0gyZok65Ks27FjxxTbkKThM+1fTlZVATWF89ZW1WhVjY6MjEy3DUkaGlMN7if3L4F0P7d39S3Aip5xy7uaJGmGTDW4bwMu6bYvAW7tqX+0u7vkTODZniUVSdIMmD/RgCQ3AWcDxyfZDPwFcBVwS5JLgV8CH+mG3wGcD2wEXgQ+Ngs9S9JQmzC4q+qicQ699yBjC7hsuk1JksbnJyclqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDVmwuBOcl2S7Uk29NQ+m2RLkvXd4/yeY1ck2ZjksSR/MluNS9Kw6ueK+3rg3IPUr66q1d3jDoAkpwIXAr/fnfPfk8ybqWYlSX0Ed1X9I/CrPp/vAuDmqtpdVb9g7Nvez5hGf5KkN5jOGvcnkjzYLaUc29WWAU/0jNnc1Q6QZE2SdUnW7dixYxptSNJwmWpwXwP8LrAa2Ar81WSfoKrWVtVoVY2OjIxMsQ1JGj5TCu6qerKq9lXVq8Df8uvlkC3Aip6hy7uaJGmGTCm4kyzt2f0QsP+Ok9uAC5MsTHISsAq4b3otSpJ6zZ9oQJKbgLOB45NsBv4CODvJaqCATcCfAVTVQ0luAR4G9gKXVdW+2WldkobThMFdVRcdpHztm4y/ErhyOk1JksbnJyclqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEudV/ft5fl/+ikv79w26FakNzXh7YDSXFX1Kv/v/9zInl1jf0Pt1X172bXtcY4/5Sx++1/82wF3J43P4NbwKti1baNX2GqOSyWS1BiDW8Mr4agTTj6g/NIz/8Te3S8MoCGpPwa3hlYSjjnxDw+ov7D95+x96fkBdCT1x+CWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxEwZ3khVJ7k7ycJKHknyyqy9OcmeSx7ufx3b1JPlSko1JHkxy+mxPQpKGST9X3HuBT1fVqcCZwGVJTgUuB+6qqlXAXd0+wHmMfbv7KmANcM2Mdy1JQ2zC4K6qrVX1o277eeARYBlwAXBDN+wG4IPd9gXAl2vMD4BFSZbOeOeSNKQmtcadZCVwGnAvsKSqtnaHtgFLuu1lwBM9p23uam98rjVJ1iVZt2PHjkm2LUnDq+/gTnIU8A3gU1X1XO+xqiqgJvPCVbW2qkaranRkZGQyp0rSUOsruJMsYCy0b6yqb3blJ/cvgXQ/t3f1LcCKntOXdzVJ0gzo566SANcCj1TVF3sO3QZc0m1fAtzaU/9od3fJmcCzPUsq0iFl3uFHctj8w19frOKpR+8ZTENSH/r5Bpz3ABcDP0myvqv9OXAVcEuSS4FfAh/pjt0BnA9sBF4EPjajHUsz6KilqzjyuBW88OTPXld/+dnt45whDd6EwV1V9wAZ5/B7DzK+gMum2Zf0lhj7B6XUFj85KUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNwaekf/1ikH1HY/t509LzwzgG6kiRncGnrHrPiDA2ovP7OVPc//agDdSBMzuCWpMQa3JDXG4JakxhjcktSYfr4seEWSu5M8nOShJJ/s6p9NsiXJ+u5xfs85VyTZmOSxJH8ymxOQpGHTz5cF7wU+XVU/SnI0cH+SO7tjV1fVF3oHJzkVuBD4feC3gO8l+b2q2jeTjUvSsJrwiruqtlbVj7rt54FHgGVvcsoFwM1VtbuqfsHYt72fMRPNSpImucadZCVwGnBvV/pEkgeTXJfk2K62DHii57TNvHnQS5Imoe/gTnIU8A3gU1X1HHAN8LvAamAr8FeTeeEka5KsS7Jux44dkzlVkoZaX8GdZAFjoX1jVX0ToKqerKp9VfUq8Lf8ejlkC7Ci5/TlXe11qmptVY1W1ejIyMh05iBJQ6Wfu0oCXAs8UlVf7Kkv7Rn2IWBDt30bcGGShUlOAlYB981cy5I03Pq5q+Q9wMXAT5Ks72p/DlyUZDVQwCbgzwCq6qEktwAPM3ZHymXeUSJJM2fC4K6qe4Ac5NAdb3LOlcCV0+hLkjQOPzmpoXfYgoXMO/w3Dqjv2eVfB9ShyeDW0Dty8TKOWrrqgPr2h+4eQDfSxAxuDb2x379L7TC4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDWmnz/rKjVp586dfPzjH+fll1+ecOyfnn4MpyxZ+Lrao48+whXXf6jv17vqqqs45ZRTJt2nNFkGt+as3bt3853vfIcXXnhhwrFnLX0/v/e2lex59Qj2/xXjHU9t59vf/nbfr/eZz3xmqq1Kk2JwS8Are19ly0sns+G591DdCuKLL36Xw3Irr1YNuDvp9VzjloD/8fe/4IGdZ7G3FrKvFrCvFnDcCafzR/982aBbkw5gcEvArpf2sq/mva62l3/GHo4bUEfS+Pr5suAjktyX5IEkDyX5XFc/Kcm9STYm+WqSw7v6wm5/Y3d85exOQZq+ednHwsNe/0vMI+ft4uj5OwfUkTS+fq64dwPnVNU7gNXAuUnOBP4SuLqqTgaeAS7txl8KPNPVr+7GSYe0I+c9z+nHfo+j5z9N7XmSp57aRD3/f3lx9yuDbk06QD9fFlzArm53Qfco4BzgX3f1G4DPAtcAF3TbAF8H/luSdM8jHZKefu4l/uZrX4N8nV9ue5b1G7cRyl9M6pDU110lSeYB9wMnA38N/AzYWVV7uyGbgf2/xVkGPAFQVXuTPAscBzw13vNv27aNz3/+81OagDSeXbt28cor/V0x73ppD9++59HX1SYb2TfeeCP33HPPJM+SDm7btm3jHusruKtqH7A6ySLgW8Dbp9tUkjXAGoBly5Zx8cUXT/cppdfZsWMHX/jCF9izZ89b8nrnnXce73znO9+S19Lc95WvfGXcY5O6j7uqdia5G3g3sCjJ/O6qezmwpRu2BVgBbE4yHzgGePogz7UWWAswOjpaJ5xwwmRakSaU5C39IuDFixfj+1gzZcGCBeMe6+eukpHuSpskRwLvAx4B7gY+3A27BLi1276t26c7/n3XtyVp5vRzxb0UuKFb5z4MuKWqbk/yMHBzkv8M/Bi4tht/LfA/k2wEfgVcOAt9S9LQ6ueukgeB0w5S/zlwxkHqLwN/OiPdSZIO4CcnJakxBrckNca/Dqg5a+HChXzgAx/o6+9xz4TFixe/Ja8jGdyasxYtWsRNN9006DakGedSiSQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqTD9fFnxEkvuSPJDkoSSf6+rXJ/lFkvXdY3VXT5IvJdmY5MEkp8/2JCRpmPTz97h3A+dU1a4kC4B7kvx9d+zfV9XX3zD+PGBV93gXcE33U5I0Aya84q4xu7rdBd2j3uSUC4Avd+f9AFiUZOn0W5UkQZ9r3EnmJVkPbAfurKp7u0NXdsshVydZ2NWWAU/0nL65q0mSZkBfwV1V+6pqNbAcOCPJHwBXAG8H/ghYDPyHybxwkjVJ1iVZt2PHjkm2LUnDa1J3lVTVTuBu4Nyq2toth+wG/g44oxu2BVjRc9ryrvbG51pbVaNVNToyMjK17iVpCPVzV8lIkkXd9pHA+4BH969bJwnwQWBDd8ptwEe7u0vOBJ6tqq2z0r0kDaF+7ipZCtyQZB5jQX9LVd2e5PtJRoAA64F/142/Azgf2Ai8CHxs5tuWpOE1YXBX1YPAaQepnzPO+AIum35rkqSD8ZOTktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMamqQfdAkueBxwbdxyw5Hnhq0E3Mgrk6L5i7c3Nebfntqho52IH5b3Un43isqkYH3cRsSLJuLs5trs4L5u7cnNfc4VKJJDXG4Jakxhwqwb120A3Mork6t7k6L5i7c3Nec8Qh8ctJSVL/DpUrbklSnwYe3EnOTfJYko1JLh90P5OV5Lok25Ns6KktTnJnkse7n8d29ST5UjfXB5OcPrjO31ySFUnuTvJwkoeSfLKrNz23JEckuS/JA928PtfVT0pyb9f/V5Mc3tUXdvsbu+MrB9n/RJLMS/LjJLd3+3NlXpuS/CTJ+iTrulrT78XpGGhwJ5kH/DVwHnAqcFGSUwfZ0xRcD5z7htrlwF1VtQq4q9uHsXmu6h5rgGveoh6nYi/w6ao6FTgTuKz736b1ue0GzqmqdwCrgXOTnAn8JXB1VZ0MPANc2o2/FHimq1/djTuUfRJ4pGd/rswL4I+ranXPrX+tvxenrqoG9gDeDXy3Z/8K4IpB9jTFeawENvTsPwYs7baXMnafOsDfABcdbNyh/gBuBd43l+YG/AbwI+BdjH2AY35Xf+19CXwXeHe3Pb8bl0H3Ps58ljMWYOcAtwOZC/PqetwEHP+G2px5L072MeilkmXAEz37m7ta65ZU1dZuexuwpNtucr7dP6NPA+5lDsytW05YD2wH7gR+Buysqr3dkN7eX5tXd/xZ4Li3tuO+/RfgM8Cr3f5xzI15ARTwD0nuT7KmqzX/XpyqQ+WTk3NWVVWSZm/dSXIU8A3gU1X1XJLXjrU6t6raB6xOsgj4FvD2Abc0bUn+JbC9qu5Pcvag+5kFZ1XVliRvA+5M8mjvwVbfi1M16CvuLcCKnv3lXa11TyZZCtD93N7Vm5pvkgWMhfaNVfXNrjwn5gZQVTuBuxlbQliUZP+FTG/vr82rO34M8PRb3Go/3gP8qySbgJsZWy75r7Q/LwCqakv3cztj/2d7BnPovThZgw7uHwKrut98Hw5cCNw24J5mwm3AJd32JYytD++vf7T7rfeZwLM9/9Q7pGTs0vpa4JGq+mLPoabnlmSku9ImyZGMrds/wliAf7gb9sZ57Z/vh4HvV7dweiipqiuqanlVrWTsv6PvV9W/ofF5AST5zSRH798G3g9soPH34rQMepEdOB/4KWPrjP9x0P1Mof+bgK3AK4ytpV3K2FrhXcDjwPeAxd3YMHYXzc+AnwCjg+7/TeZ1FmPrig8C67vH+a3PDfhD4MfdvDYA/6mr/w5wH7AR+BqwsKsf0e1v7I7/zqDn0McczwZunyvz6ubwQPd4aH9OtP5enM7DT05KUmMGvVQiSZokg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMb8fxGLn6naSuvgAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"vuwjvzf3mdpY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927548453,"user_tz":-120,"elapsed":1397,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":[""],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypqn8Tv7cvq6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600927548454,"user_tz":-120,"elapsed":1370,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"outputId":"8faefd55-e305-4aef-ed78-6a6e7576588f"},"source":["print(state_dim[0])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uqRBpsKPQPny","colab_type":"text"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"7Tlm8pZ5QPnz","colab_type":"text"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__.\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"_HEfeF5mQPnz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927549260,"user_tz":-120,"elapsed":2143,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["import torch\n","import torch.nn as nn"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"mEQxigrOQPn3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927549264,"user_tz":-120,"elapsed":2131,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["# Build a simple neural network that predicts policy logits. \n","# Keep it simple: CartPole isn't worth deep architectures.\n","model = nn.Sequential(\n","  # <YOUR CODE: define a neural network that predicts policy logits>\n","          nn.Linear(state_dim[0],200),\n","          nn.ReLU(), \n","          nn.Linear(200,100),\n","          nn.ReLU(),\n","          nn.Linear(100,n_actions),\n","          # nn.ReLU(),\n","          # nn.Conv2d(20,64,5),\n","          # nn.ReLU()\n",")"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WO1aQSw8QPn6","colab_type":"text"},"source":["#### Predict function"]},{"cell_type":"markdown","metadata":{"id":"9yRpGuOoQPn7","colab_type":"text"},"source":["Note: output value of this function is not a torch tensor, it's a numpy array.\n","So, here gradient calculation is not needed.\n","<br>\n","Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n","to suppress gradient calculation.\n","<br>\n","Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n","<br>\n","With `.detach()` computational graph is built but then disconnected from a particular tensor,\n","so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n","<br>\n","In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."]},{"cell_type":"code","metadata":{"id":"KURGYoFJQPn7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927549267,"user_tz":-120,"elapsed":2120,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["from scipy.special import softmax\n","\n","def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    # convert states, compute logits, use softmax to get probability\n","    # print(type(states))\n","    softmax=nn.Softmax(dim=1)#.to_numpy()\n","\n","    with torch.no_grad():\n","      probs=softmax(model(torch.from_numpy(states).float()))\n","      # print(type(probs))\n","      # print(logits)\n","      probs=np.array(probs)\n","      # print(logits)\n","      # probs.output\n","\n","    # probs=np.exp(logits)/np.sum(np.exp(logits),axis=0)\n","    #  = nn.Softmax()(probs)\n","    # <YOUR CODE>\n","    # print(np.sum(np.exp(logits),axis=0))\n","    # print(np.exp(logits))\n","    # print(np.round(probs,1))\n","    # probs =softmax(logits, axis)\n","    return probs"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"GplE2aupQPn_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927549267,"user_tz":-120,"elapsed":2101,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["test_states = np.array([env.reset() for _ in range(5)])\n","test_probas = predict_probs(test_states)\n","assert isinstance(test_probas, np.ndarray), \\\n","    \"you must return np array and not %s\" % type(test_probas)\n","assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n","    \"wrong output shape: %s\" % np.shape(test_probas)\n","\n","assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UTYpCfX4QPoC","colab_type":"text"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"Hcc1AC6An2qj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600927549268,"user_tz":-120,"elapsed":2084,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"outputId":"448e1462-ae04-463c-8d7a-6186ba7191b3"},"source":["env.action_space.contains(0)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"euzJUhirQPoD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927549268,"user_tz":-120,"elapsed":2071,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["import random\n","def generate_session(env, t_max=1000):\n","    \"\"\" \n","    Play a full session with REINFORCE agent.\n","    Returns sequences of states, actions, and rewards.\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(np.array([s]))[0]\n","        # print(action_probs)\n","        # Sample action with given probabilities.\n","        a = random.choices(np.array([0,1]),weights=action_probs)\n","        # print(type(a))\n","        new_s, r, done, info = env.step(a[0])\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"i1UeLQmnQPoI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927549269,"user_tz":-120,"elapsed":2059,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["# test it\n","states, actions, rewards = generate_session(env)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dfJIClr0QPoM","colab_type":"text"},"source":["### Computing cumulative rewards\n","\n","$$\n","\\begin{align*}\n","G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n","&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n","&= r_t + \\gamma * G_{t + 1}\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"XElR_NubQPoN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927549269,"user_tz":-120,"elapsed":2047,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["\n","def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    # powers=np.arrange(len(rewards))\n","    # print(rewards)\n","    comu_rewards=np.zeros(shape=len(rewards))\n","    for i in range(len(rewards)-1,-1,-1):\n","      # print(i)\n","      # print(len(rewards))\n","      if i ==len(rewards)-1:\n","        comu_rewards[i]=rewards[i]*gamma\n","        # print(comu_rewards[i])\n","      else:\n","        comu_rewards[i]=rewards[i]+comu_rewards[i+1]*gamma\n","        # print(comu_rewards[i])\n","\n","\n","    # for i,reward in enumerate(rewards):\n","      # comu_rewards[i]=reward+rewards[i:]*\n","\n","\n","    # gamma_list=np.array([gamma**f for f in range(len(rewards))])\n","    # g=sum([])\n","    # <YOUR CODE>\n","    # print(comu_rewards)\n","    return comu_rewards#YOUR CODE: array of cumulative rewards>"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4qL68LAQPoR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600927549270,"user_tz":-120,"elapsed":2034,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"outputId":"2eff0eb1-a948-451d-aec4-31d4a542c904"},"source":["# get_cumulative_rewards(rewards)\n","assert len(get_cumulative_rewards(list(range(100)))) == 100\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","    [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8klp5rOxQPoU","colab_type":"text"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n","\n","$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n","\n","$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."]},{"cell_type":"code","metadata":{"id":"dCSCxJrjQPoV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927549270,"user_tz":-120,"elapsed":2013,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["def to_one_hot(y_tensor, ndims):\n","    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n","    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n","    y_one_hot = torch.zeros(\n","        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n","    return y_one_hot"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"7az3bKmgQPoY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927549270,"user_tz":-120,"elapsed":1993,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["# Your code: define optimizers\n","optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n","\n","\n","def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n","    \"\"\"\n","    Takes a sequence of states, actions and rewards produced by generate_session.\n","    Updates agent's weights by following the policy gradient above.\n","    Please use Adam optimizer with default parameters.\n","    \"\"\"\n","\n","    # cast everything into torch tensors\n","    states = torch.tensor(states, dtype=torch.float32)\n","    actions = torch.tensor(actions, dtype=torch.int32)\n","    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n","    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n","\n","    # predict logits, probas and log-probas using an agent.\n","    logits = model(states)\n","    # print(\"*\"*80)\n","    # print(logits)\n","    probs = nn.functional.softmax(logits, -1)\n","    # print(\"*\"*80)\n","    # print(probs)\n","    log_probs = nn.functional.log_softmax(logits, -1)\n","    # print(\"*\"*80)\n","    # print(log_probs)\n","\n","    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n","        \"please use compute using torch tensors and don't use predict_probs function\"\n","\n","    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n","    log_probs_for_actions = torch.sum(\n","        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n","    # print(\"*\"*80)\n","    # print(to_one_hot(actions, env.action_space.n))\n","    # print(\"*\"*80)\n","    # print(log_probs_for_actions)\n","\n","    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n","    # entropy =nn.CrossEntropyLoss()\n","    # print(actions.shape)\n","    # print(states.shape)\n","    # log_probs_for_actions = log_probs_for_actions.view(-1)\n","    # cumulative_returns = cumulative_returns.view(-1)\n","    # print(log_probs_for_actions[:])\n","    # print(cumulative_returns[:])\n","    # m = Categorical(probs)\n","    # action = m.sample()\n","    #     return action.item(), m.log_prob(action)\n","    # print(cumulative_returns.shape)\n","    # print(log_probs_for_actions.shape)\n","\n","    loss = torch.sum(\n","        -log_probs_for_actions *cumulative_returns )\n","    # loss=log_probs_for_actions* cumulative_returns\n","    # print(\"here\")\n","    # loss = nn.criterion(loss)\n","    # loss.backward()\n","\n","    # Gradient descent step\n","    optimizer.zero_grad() \n","    loss.backward()\n","    optimizer.step()\n","\n","\n","    # technical: return session rewards to print them later\n","    return np.sum(rewards)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMM0H_ioQPob","colab_type":"text"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"id":"W1l2wywsQPoc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1600927568241,"user_tz":-120,"elapsed":20947,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"outputId":"c40019a2-7034-4c6e-fee7-f496fded6b16"},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n","    \n","    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n","    \n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":16,"outputs":[{"output_type":"stream","text":["mean reward:41.390\n","mean reward:163.920\n","mean reward:233.580\n","mean reward:134.510\n","mean reward:381.890\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aw2TBI2dQPoh","colab_type":"text"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"fwt67mNHQPoh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600927580836,"user_tz":-120,"elapsed":33521,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4KD_sEkQPok","colab_type":"code","colab":{"resources":{"http://localhost:8080/videos/openaigym.video.0.1154.video000064.mp4":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""}},"base_uri":"https://localhost:8080/","height":500},"executionInfo":{"status":"ok","timestamp":1600927580838,"user_tz":-120,"elapsed":28217,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"outputId":"34114aac-6c70-4d7d-a845-1351e67a2fb6"},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"videos/openaigym.video.0.1154.video000064.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"eOSzcyMUQPon","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1600927592820,"user_tz":-120,"elapsed":11969,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"outputId":"e4201aee-df8c-404e-ea23-09ac11700896"},"source":["from submit import submit_cartpole\n","submit_cartpole(generate_session, 'zeyad.3ezzat@gmail.com', 'etgdJBA2s91uUxIa')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Your average reward is 550.74 over 100 episodes\n","Submitted to Coursera platform. See results on assignment page!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gE_ZNrceQPoq","colab_type":"text"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}]}